{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8130934,"sourceType":"datasetVersion","datasetId":4555568},{"sourceId":8965827,"sourceType":"datasetVersion","datasetId":5396996},{"sourceId":8965953,"sourceType":"datasetVersion","datasetId":5397075},{"sourceId":77903,"sourceType":"modelInstanceVersion","modelInstanceId":65491,"modelId":90156}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Digital Audio Forensics using AI - Detecting Deepfake Audio","metadata":{"id":"lLpMWXqZQAF7"}},{"cell_type":"markdown","source":"## Envirtonment Setup","metadata":{"id":"JFeP0-OJLT5n"}},{"cell_type":"code","source":"#Following libraries are needed for thie notebook\n\n!pip install torch\n!pip install torchaudio\n!pip install librosa\n!pip install numpy\n!pip install matplotlib\n!pip install tqdm\n!pip install IPython\n!pip install torchvision\n","metadata":{"id":"y8R2Q1j-Tef9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{"id":"xkXChIzAL3aY"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport IPython\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, ConcatDataset\nfrom torchvision import transforms\nfrom torchaudio import transforms as AT\nfrom torch.nn import functional as F\nfrom torch import flatten\nfrom torch import nn\nimport torch.optim as optim\nimport torchaudio\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve","metadata":{"id":"1dnHhXzyLfnK","execution":{"iopub.status.busy":"2024-07-21T09:55:26.064215Z","iopub.execute_input":"2024-07-21T09:55:26.064933Z","iopub.status.idle":"2024-07-21T09:55:26.075929Z","shell.execute_reply.started":"2024-07-21T09:55:26.064880Z","shell.execute_reply":"2024-07-21T09:55:26.074772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset used:","metadata":{"id":"1PRZvWATUjH1"}},{"cell_type":"markdown","source":"**The following data set has been used:**\n\n[The Fake-or-Real (FoR) Dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)\n\nA private dataset has also been used that was made using ElevenLabs text to speech model.\n","metadata":{"id":"iZbutY8kWcj4"}},{"cell_type":"markdown","source":"## Dataset creation class:","metadata":{"id":"fgLUeM78z5yN"}},{"cell_type":"code","source":"\"\"\"Create a DataSet object from the Fake or Real Dataset. Return the MFCC of audio file along with\nthe corresponding label. 0:Fake and 1:Real\"\"\"\n\nn_fft = 2048\nwin_length = None\nhop_length = 512\nn_mels = 256\n\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transform=None, target_sample_rate=16000,target_time_length = 972):\n        super().__init__()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.target_sample_rate = target_sample_rate\n        self.target_time_length = target_time_length\n        self.classes = os.listdir(root_dir)\n        self.file_paths = []\n        self.labels = []\n\n        #class_idx: {0:fake, 1:real}\n        for class_idx, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            for file_name in tqdm(os.listdir(class_dir)):\n                file_path = os.path.join(class_dir, file_name)\n                self.file_paths.append(file_path)\n                self.labels.append(class_idx)\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def mix_down_if_necessary(self, signal): #to convert from stereo to mono\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim = 0, keepdims = True)\n        return signal\n\n    def __getitem__(self, idx):\n        try:\n            file_path = self.file_paths[idx]\n            label = self.labels[idx]\n\n            # Load audio file using torchaudio\n            waveform, sample_rate = torchaudio.load(file_path)\n            waveform = self.mix_down_if_necessary(waveform)\n            if sample_rate != self.target_sample_rate:\n                resampler_transform = AT.Resample(sample_rate,self.target_sample_rate)\n                waveform = resampler_transform(waveform)\n                sample_rate = self.target_sample_rate\n\n            # Apply transformation to get MFCC\n            if self.transform:\n                mfcc = self.transform(waveform)\n                if mfcc.shape[2] != self.target_time_length:\n                    mfcc = torch.nn.functional.pad(mfcc, (0, self.target_time_length - mfcc.shape[2], 0, 0), mode='constant')\n                else:\n                    mfcc = mfcc[:, :self.target_time_length]\n\n            return mfcc,label\n\n        except Exception as e:\n            return self.__getitem__(idx + 1)\n\n\n#mfcc transform that gives us the MFCC tensor\nmfcc_transform = AT.MFCC(\n    sample_rate=16000,\n    n_mfcc=40,\n    melkwargs={\n        \"n_fft\": n_fft,\n        \"n_mels\": n_mels,\n        \"hop_length\": hop_length,\n        \"mel_scale\": \"htk\",\n    },\n)\n","metadata":{"id":"Cf1oYDdFQM-h","execution":{"iopub.status.busy":"2024-07-21T09:55:28.664911Z","iopub.execute_input":"2024-07-21T09:55:28.665288Z","iopub.status.idle":"2024-07-21T09:55:28.761266Z","shell.execute_reply.started":"2024-07-21T09:55:28.665260Z","shell.execute_reply":"2024-07-21T09:55:28.760401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Class:","metadata":{"id":"nIhrFmEEVE5H"}},{"cell_type":"code","source":"#nueral network model class\nclass ShallowCNN(nn.Module):\n    def __init__(self, in_features = 1, out_dim=1, **kwargs):\n        super(ShallowCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_features, 32, kernel_size=4, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 48, kernel_size=5, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(48, 64, kernel_size=4, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=(2, 4), stride=1, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(15104, 128)\n        self.fc2 = nn.Linear(128, out_dim)\n\n    def forward(self, x: torch.Tensor):\n        x = x.unsqueeze(1)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        x = flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"id":"S9cueRoRVMW5","execution":{"iopub.status.busy":"2024-07-21T09:55:31.307146Z","iopub.execute_input":"2024-07-21T09:55:31.307510Z","iopub.status.idle":"2024-07-21T09:55:31.317202Z","shell.execute_reply.started":"2024-07-21T09:55:31.307482Z","shell.execute_reply":"2024-07-21T09:55:31.316247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Dataset objects:","metadata":{"id":"6wg4BKlVc1-b"}},{"cell_type":"code","source":"dataset_fake_or_real_for_2sec_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/testing',transform=mfcc_transform)\ndataset_fake_or_real_for_2sec_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/training',transform=mfcc_transform)\ndataset_fake_or_real_for_2sec_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/validation',transform=mfcc_transform)\n\ndataset_fake_or_real_for_norm_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-norm/for-norm/testing',transform=mfcc_transform)\ndataset_fake_or_real_for_norm_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-norm/for-norm/training',transform=mfcc_transform)\ndataset_fake_or_real_for_norm_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-norm/for-norm/validation',transform=mfcc_transform)\n\ndataset_fake_or_real_for_original_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-original/for-original/testing',transform=mfcc_transform)\ndataset_fake_or_real_for_original_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-original/for-original/training',transform=mfcc_transform)\ndataset_fake_or_real_for_original_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-original/for-original/validation',transform=mfcc_transform)\n\ndataset_fake_or_real_for_rerec_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-rerec/for-rerecorded/testing',transform=mfcc_transform)\ndataset_fake_or_real_for_rerec_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-rerec/for-rerecorded/training',transform=mfcc_transform)\ndataset_fake_or_real_for_rerec_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-rerec/for-rerecorded/validation',transform=mfcc_transform)\n\n\ndataset_private_train = CustomDataset(root_dir = '/kaggle/input/dataset/trainData',transform=mfcc_transform)\ndataset_private_test = CustomDataset(root_dir = '/kaggle/input/dataset/testData',transform=mfcc_transform)\n\nprototype_phase_data = CustomDataset(root_dir = '/kaggle/input/privatedata/Prototype Assignment', transform = mfcc_transform)\n\ndataset_train = ConcatDataset([\n    dataset_fake_or_real_for_2sec_training,\n    dataset_fake_or_real_for_norm_training,\n    dataset_fake_or_real_for_original_training,\n    dataset_fake_or_real_for_rerec_training,\n    dataset_private_train\n])\n\ndataset_val = ConcatDataset([\n    dataset_fake_or_real_for_2sec_validation,\n    dataset_fake_or_real_for_norm_validation,\n    dataset_fake_or_real_for_original_validation,\n    dataset_fake_or_real_for_rerec_validation,\n\n])\n\ndataset_test = ConcatDataset([\n    dataset_fake_or_real_for_2sec_testing,\n    dataset_fake_or_real_for_norm_testing,\n    dataset_fake_or_real_for_original_testing,\n    dataset_fake_or_real_for_rerec_testing,\n    dataset_private_test\n\n])","metadata":{"id":"aplWLtaBi2_e","execution":{"iopub.status.busy":"2024-07-21T09:56:50.194638Z","iopub.execute_input":"2024-07-21T09:56:50.195393Z","iopub.status.idle":"2024-07-21T09:56:51.195531Z","shell.execute_reply.started":"2024-07-21T09:56:50.195362Z","shell.execute_reply":"2024-07-21T09:56:51.194717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating iterable Dataloader object from datasets:","metadata":{"id":"ApgifimQSMRb"}},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset_train, shuffle = True, batch_size = 32, num_workers=4)\nval_loader = torch.utils.data.DataLoader(dataset_val, shuffle = True, batch_size = 32,num_workers=4)\ntest_loader = torch.utils.data.DataLoader(dataset_test, shuffle = True, batch_size = 32,num_workers=4)","metadata":{"id":"M8Y8a54XQAGD","execution":{"iopub.status.busy":"2024-07-21T09:56:56.230469Z","iopub.execute_input":"2024-07-21T09:56:56.230826Z","iopub.status.idle":"2024-07-21T09:56:56.243485Z","shell.execute_reply.started":"2024-07-21T09:56:56.230797Z","shell.execute_reply":"2024-07-21T09:56:56.242622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graphs of different Audio features:","metadata":{"id":"8_42rW-Ia2Ln"}},{"cell_type":"code","source":"#function to plot audio waveform\ndef plot_waveform(waveform, label, sr):\n    waveform = waveform.detach().numpy()\n    plt.figure(figsize= (6,2))\n    plt.xlabel('Time')\n    plt.ylabel('Amplitude')\n    plt.plot(waveform)\n    plt.title('Real Audio Waveform' if label == 1 else 'Fake Audio Waveform')\n    plt.show()\n\n#fucntion to plot Mel Spectrogram of Audio file\ndef plot_spectrogram(waveform,label, sr):\n    waveform = waveform.detach().numpy()\n    n_fft = 2048\n    hop_length = 512\n    win_length = 2048\n    n_mels = 128\n\n    spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length, n_mels=n_mels)\n    log_spectrogram = librosa.power_to_db(spectrogram)\n\n    plt.figure(figsize=(10, 4))\n    plt.imshow(log_spectrogram, cmap='viridis', aspect='auto', origin='lower')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Real Audio Spectrogram' if label == 1 else 'Fake Audio Spectrogram')\n    plt.xlabel('Time')\n    plt.ylabel('Frequency')\n    plt.show()\n\n#function to plot MFCC of audio file\ndef plot_mfcc(waveform,label,sr):\n    #mfcc transform that gives us the MFCC tensor\n    n_fft = 2048\n    win_length = None\n    hop_length = 512\n    n_mels = 256\n    mfcc_transform = AT.MFCC(\n        sample_rate=16000,\n        n_mfcc=40,\n        melkwargs={\n            \"n_fft\": n_fft,\n            \"n_mels\": n_mels,\n            \"hop_length\": hop_length,\n            \"mel_scale\": \"htk\",\n        },\n    )\n    mfcc = mfcc_transform(waveform)\n    if mfcc.shape[1] != 972:\n        mfcc = torch.nn.functional.pad(mfcc, (0, 972 - mfcc.shape[1], 0, 0), mode='constant')\n    else:\n        mfcc = mfcc[:, :972]\n    mfcc = mfcc.detach().numpy()\n    log_mfcc = librosa.power_to_db(mfcc)\n\n    plt.figure(figsize=(10, 4))\n    plt.imshow(log_mfcc, cmap='viridis', aspect='auto', origin='lower')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Real Audio MFCC' if label == 1 else 'Fake Audio MFCC')\n    plt.xlabel('Time')\n    plt.ylabel('MFCC')\n    plt.show()\n\ndef plot_graphs(file_path,label): #0 for fake and 1 for real\n    waveform, sample_rate = torchaudio.load(file_path)\n    #to convert stereo to mono\n    if waveform.shape[0] > 1:\n        waveform = torch.mean(signal, dim = 0, keepdims = True)\n\n    #calling the ploting functions\n    plot_waveform(waveform[0],label,16000)\n    plot_spectrogram(waveform[0],label,16000)\n    plot_mfcc(waveform[0],label,16000)\n\nfile_path = \"/kaggle/input/privatedata/Prototype Assignment/fake/Fake_1.wav\"\nplot_graphs(file_path,0)","metadata":{"id":"5f0uto7hbFd1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training setup:","metadata":{"id":"JtxX5yecY7IE"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#clear cache befor starting the training\ntorch.cuda.empty_cache()\nmodel = ShallowCNN(in_features=1, out_dim=1).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nnum_epochs = 5","metadata":{"id":"ghFJI2SwQAGE","execution":{"iopub.status.busy":"2024-07-21T09:57:02.699703Z","iopub.execute_input":"2024-07-21T09:57:02.700319Z","iopub.status.idle":"2024-07-21T09:57:02.730447Z","shell.execute_reply.started":"2024-07-21T09:57:02.700287Z","shell.execute_reply":"2024-07-21T09:57:02.729744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training and validation loop\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0.0\n    loop = tqdm(enumerate(train_loader), total=len(train_loader))\n    for batch_idx, (batch_mfccs, batch_labels) in loop:\n        loop.set_description(f'Epoch {epoch + 1} / {num_epochs}')\n        batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n\n        # Zero the gradients\n        optimizer.zero_grad()\n        batch_mfccs = batch_mfccs.squeeze(1)\n\n        # Forward pass\n        outputs = model(batch_mfccs)\n\n        # Compute loss\n        loss = criterion(outputs, batch_labels)\n\n        # Backward pass\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        # Accumulate training loss\n        train_loss += loss.item()\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (batch_mfccs, batch_labels) in enumerate(val_loader):\n            batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n\n            # Forward pass\n            batch_mfccs = batch_mfccs.squeeze(1)\n            outputs = model(batch_mfccs)\n\n            # Compute loss\n            loss = criterion(outputs, batch_labels)\n\n            # Accumulate validation loss\n            val_loss += loss.item()\n            batch_pred = (torch.sigmoid(outputs) + 0.5).int()\n            # Compute accuracy\n            total += batch_labels.size(0)\n            correct += (batch_pred == batch_labels).sum().item()\n\n    # Print epoch statistics\n    train_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n    val_accuracy = correct / total\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2%}\")\n\nprint(\"Training finished.\")\n\n","metadata":{"id":"sIjimpmFY_k3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the model weights in a file:","metadata":{"id":"wAVH1UrMUC-r"}},{"cell_type":"code","source":"model_path = 'FinalDeliverableModel.pth'\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to '{model_path}'.\")","metadata":{"id":"lFrVszS3UGsc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the model:","metadata":{"id":"s0w9wL9uUWeW"}},{"cell_type":"code","source":"model.eval()\nval_loss = 0.0\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for batch_idx, (batch_mfccs, batch_labels) in tqdm(enumerate(test_loader)):\n        batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n\n        # Forward pass\n        batch_mfccs = batch_mfccs.squeeze(1)\n        outputs = model(batch_mfccs)\n\n        # Compute loss\n        loss = criterion(outputs, batch_labels)\n\n        # Accumulate validation loss\n        val_loss += loss.item()\n        batch_pred = (torch.sigmoid(outputs) + 0.5).int()\n        # Compute accuracy\n        total += batch_labels.size(0)\n        correct += (batch_pred == batch_labels).sum().item()\nprint(f\"Loss is {val_loss}\")\nprint(correct)\nprint(total)\nprint(f\"Accuracy is {correct/total}\")","metadata":{"id":"-g3Lp-raQAGG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the model on prototype phase data:","metadata":{"id":"x0DrjhN7VAz3"}},{"cell_type":"code","source":"prototype_phase_test_loader = torch.utils.data.DataLoader(prototype_phase_data, batch_size = 1)\n\ndef compute_eer(y_true, y_scores):\n    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n    fnr = 1 - tpr\n    eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    return eer, eer_threshold\n\ndef inference_function(prototype_phase_test_loader):\n    model.eval()\n    all_labels = []\n    all_predictions = []\n    all_scores = []\n\n    with torch.no_grad():\n        for batch_idx, (batch_mfccs, batch_labels) in tqdm(enumerate(prototype_phase_test_loader)):\n            batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n\n            # Forward pass\n            batch_mfccs = batch_mfccs.squeeze(1)\n            outputs = model(batch_mfccs)\n#             print(outputs.item())\n            scores = torch.sigmoid(outputs).cpu().numpy()\n            predicted = (torch.sigmoid(outputs) + 0.5).int()\n    \n            all_labels.extend(batch_labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n            all_scores.extend(scores)\n\n  \n            #print(f\"For {batch_idx} we have the label {batch_labels.item()} and the predicted output is {predicted.item()}\")\n    \n    #conver lists to numpy arrays\n    all_labels = np.array(all_labels)\n    all_predictions = np.array(all_predictions)\n    all_scores = np.array(all_scores)\n    print(len(all_scores))\n    print(len(all_labels))\n    if len(all_labels) == 0 or len(all_scores) == 0:\n        raise ValueError(\"No labels or scores were collected. Check the data loading and model inference steps.\")\n\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n    f1 = f1_score(all_labels, all_predictions)\n    roc_auc = roc_auc_score(all_labels, all_predictions)\n    conf_matrix = confusion_matrix(all_labels, all_predictions)\n    eer, eer_threshold = compute_eer(all_labels, all_scores)\n\n    print(f'Accuracy: {accuracy:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n    print(f'F1 Score: {f1:.4f}')\n    print(f'ROC AUC: {roc_auc:.4f}')\n    print(f'EER: {eer:.4f}')\n    print('Confusion Matrix:')\n    print(conf_matrix)\n    \n    #plot Confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    # Plot ROC curve\n    fpr, tpr, _ = roc_curve(all_labels, all_scores)\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, marker='.')\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show()\n    \n    precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n    average_precision = average_precision_score(all_labels, all_scores)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(recall, precision, marker='.')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(f'Precision-Recall Curve (AP = {average_precision:.2f})')\n    plt.show()\ninference_function(test_loader)","metadata":{"id":"jlCYlxmeQAGG","execution":{"iopub.status.busy":"2024-07-21T09:58:05.847436Z","iopub.execute_input":"2024-07-21T09:58:05.847795Z","iopub.status.idle":"2024-07-21T09:59:08.310084Z","shell.execute_reply.started":"2024-07-21T09:58:05.847767Z","shell.execute_reply":"2024-07-21T09:59:08.308798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference on a single file:","metadata":{"id":"UEejq2EKiGBl"}},{"cell_type":"code","source":"'''For inference on a single file make sure the weights files is in the same directory as the notebook.\n And run the first two cells to import libraries and the Model class cell to define the model'''\n# from google.colab import files\n\n\nmodel = ShallowCNN(in_features=1, out_dim=1).to(device)\nstate_dict = torch.load('/kaggle/input/fyp_final_deliverable_model_weights/pytorch/model.pth/1/FinalDeliverableModel_bc200401244.pth')\nmodel.load_state_dict(state_dict)\nprint(\"model loaded Successfuly!\")\nn_fft = 2048\nwin_length = None\nhop_length = 512\nn_mels = 256\n\n#mfcc transform that gives us the MFCC tensor\nmfcc_transform = AT.MFCC(\n    sample_rate=16000,\n    n_mfcc=40,\n    melkwargs={\n        \"n_fft\": n_fft,\n        \"n_mels\": n_mels,\n        \"hop_length\": hop_length,\n        \"mel_scale\": \"htk\",\n    },\n)\n\n#Function to preprocess audio data:\ndef preprocess_audio(file_path,transform = mfcc_transform, target_time_length = 972):\n    waveform, sample_rate = torchaudio.load(file_path)\n    waveform = mix_down_if_necessary(waveform)\n    if sample_rate != 16000:\n        resampler_transform = AT.Resample(sample_rate,16000)\n        waveform = resampler_transform(waveform)\n        sample_rate = 16000\n\n    # Apply MFCC transformation\n    if transform:\n        mfcc = transform(waveform)\n        if mfcc.shape[2] != target_time_length:\n            mfcc = torch.nn.functional.pad(mfcc, (0, target_time_length - mfcc.shape[2], 0, 0), mode='constant')\n        else:\n            mfcc = mfcc[:, :target_time_length]\n\n    return mfcc\n\ndef mix_down_if_necessary(signal): #converting from stereo to mono\n    if signal.shape[0] > 1:\n        signal = torch.mean(signal, dim = 0, keepdims = True)\n    return signal\n\n# Prompt user to upload audio file. NOTE: Below code is only for google colab.\n# print(\"Upload an audio file:\")\n# uploaded = files.upload()\n# file_path = next(iter(uploaded.keys()))\nfile_path = '/kaggle/input/privatedata/Prototype Assignment/fake/Fake_9.wav'\n# Preprocess the provided file\nmfccs_tensor = preprocess_audio(file_path)\nmfccs_tensor = mfccs_tensor.cuda()\n\n# Perform inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(mfccs_tensor)\n    print(output.item())\n\n    predicted = (torch.sigmoid(output) + 0.5).int()\n\n# Display prediction\nif predicted.item() == 1:\n    print(f\"The model predicts that file is real.\")\nelse:\n    print(f\"The model predicts that file is AI-generated.\")\n","metadata":{"id":"Uad4xj0_QAGJ","outputId":"c4a54382-a774-4d5c-f06b-747f7059f1f2","execution":{"iopub.status.busy":"2024-07-21T09:57:19.085941Z","iopub.execute_input":"2024-07-21T09:57:19.086303Z","iopub.status.idle":"2024-07-21T09:57:20.061830Z","shell.execute_reply.started":"2024-07-21T09:57:19.086277Z","shell.execute_reply":"2024-07-21T09:57:20.060866Z"},"trusted":true},"execution_count":null,"outputs":[]}]}