{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpMWXqZQAF7"
      },
      "source": [
        "# Digital Audio Forensics using AI - Detecting Deepfake Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFeP0-OJLT5n"
      },
      "source": [
        "##Envirtonment Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Following libraries are needed for thie notebook\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install IPython\n",
        "!pip install torchvision\n"
      ],
      "metadata": {
        "id": "y8R2Q1j-Tef9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkXChIzAL3aY"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dnHhXzyLfnK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import IPython\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchaudio import transforms as AT\n",
        "from torch.nn import functional as F\n",
        "from torch import flatten\n",
        "from torch import nn\n",
        "from torch.utils.data import ConcatDataset\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset used:"
      ],
      "metadata": {
        "id": "1PRZvWATUjH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following data set has been used:**\n",
        "\n",
        "[The Fake-or-Real (FoR) Dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)\n",
        "\n",
        "A private dataset has also been used that was made using ElevenLabs text to speech model.\n"
      ],
      "metadata": {
        "id": "iZbutY8kWcj4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgLUeM78z5yN"
      },
      "source": [
        "## Dataset creation class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf1oYDdFQM-h",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"Create a DataSet object from the Fake or Real Dataset. Return the MFCC of audio file along with\n",
        "the corresponding label. 0:Fake and 1:Real\"\"\"\n",
        "\n",
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 256\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_sample_rate=16000,target_time_length = 972):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.target_time_length = target_time_length\n",
        "        self.classes = os.listdir(root_dir)\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        #class_idx: {0:fake, 1:real}\n",
        "        for class_idx, class_name in enumerate(self.classes):\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            for file_name in tqdm(os.listdir(class_dir)):\n",
        "                file_path = os.path.join(class_dir, file_name)\n",
        "                self.file_paths.append(file_path)\n",
        "                self.labels.append(class_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def mix_down_if_necessary(self, signal): #to convert from stereo to mono\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim = 0, keepdims = True)\n",
        "        return signal\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            file_path = self.file_paths[idx]\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            # Load audio file using torchaudio\n",
        "            waveform, sample_rate = torchaudio.load(file_path)\n",
        "            waveform = self.mix_down_if_necessary(waveform)\n",
        "            if sample_rate != self.target_sample_rate:\n",
        "                resampler_transform = AT.Resample(sample_rate,self.target_sample_rate)\n",
        "                waveform = resampler_transform(waveform)\n",
        "                sample_rate = self.target_sample_rate\n",
        "\n",
        "            # Apply transformation to get MFCC\n",
        "            if self.transform:\n",
        "                mfcc = self.transform(waveform)\n",
        "                if mfcc.shape[2] != self.target_time_length:\n",
        "                    mfcc = torch.nn.functional.pad(mfcc, (0, self.target_time_length - mfcc.shape[2], 0, 0), mode='constant')\n",
        "                else:\n",
        "                    mfcc = mfcc[:, :self.target_time_length]\n",
        "\n",
        "            return mfcc,label\n",
        "\n",
        "        except Exception as e:\n",
        "            return self.__getitem__(idx + 1)\n",
        "\n",
        "\n",
        "#mfcc transform that gives us the MFCC tensor\n",
        "mfcc_transform = AT.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=40,\n",
        "    melkwargs={\n",
        "        \"n_fft\": n_fft,\n",
        "        \"n_mels\": n_mels,\n",
        "        \"hop_length\": hop_length,\n",
        "        \"mel_scale\": \"htk\",\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIhrFmEEVE5H"
      },
      "source": [
        "##Model Class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9cueRoRVMW5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#nueral network model class\n",
        "class ShallowCNN(nn.Module):\n",
        "    def __init__(self, in_features = 1, out_dim=1, **kwargs):\n",
        "        super(ShallowCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_features, 32, kernel_size=4, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 48, kernel_size=5, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(48, 64, kernel_size=4, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(2, 4), stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(15104, 128)\n",
        "        self.fc2 = nn.Linear(128, out_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Dataset objects:"
      ],
      "metadata": {
        "id": "6wg4BKlVc1-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aplWLtaBi2_e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset_fake_or_real_for_2sec_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/testing',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_2sec_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/training',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_2sec_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/validation',transform=mfcc_transform)\n",
        "\n",
        "dataset_fake_or_real_for_norm_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-norm/for-norm/testing',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_norm_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-norm/for-norm/training',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_norm_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-norm/for-norm/validation',transform=mfcc_transform)\n",
        "\n",
        "dataset_fake_or_real_for_original_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-original/for-original/testing',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_original_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-original/for-original/training',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_original_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-original/for-original/validation',transform=mfcc_transform)\n",
        "\n",
        "dataset_fake_or_real_for_rerec_testing = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-rerec/for-rerecorded/testing',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_rerec_training = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-rerec/for-rerecorded/training',transform=mfcc_transform)\n",
        "dataset_fake_or_real_for_rerec_validation = CustomDataset(root_dir = '/kaggle/input/the-fake-or-real-dataset/for-rerec/for-rerecorded/validation',transform=mfcc_transform)\n",
        "\n",
        "\n",
        "dataset_private_train = CustomDataset(root_dir = '/kaggle/input/privatedata/dataSet/trainData',transform=mfcc_transform)\n",
        "dataset_private_test = CustomDataset(root_dir = '/kaggle/input/privatedata/dataSet/testData',transform=mfcc_transform)\n",
        "\n",
        "prototype_phase_data = CustomDataset(root_dir = '/kaggle/input/privatedata/Prototype Assignment', transform = mfcc_transform)\n",
        "\n",
        "dataset_train = ConcatDataset([\n",
        "    dataset_fake_or_real_for_2sec_training,\n",
        "    dataset_fake_or_real_for_norm_training,\n",
        "    dataset_fake_or_real_for_original_training,\n",
        "    dataset_fake_or_real_for_rerec_training,\n",
        "    dataset_private_train\n",
        "])\n",
        "\n",
        "dataset_val = ConcatDataset([\n",
        "    dataset_fake_or_real_for_2sec_validation,\n",
        "    dataset_fake_or_real_for_norm_validation,\n",
        "    dataset_fake_or_real_for_original_validation,\n",
        "    dataset_fake_or_real_for_rerec_validation,\n",
        "\n",
        "])\n",
        "\n",
        "dataset_test = ConcatDataset([\n",
        "    dataset_fake_or_real_for_2sec_testing,\n",
        "    dataset_fake_or_real_for_norm_testing,\n",
        "    dataset_fake_or_real_for_original_testing,\n",
        "    dataset_fake_or_real_for_rerec_testing,\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating iterable Dataloader object from datasets:"
      ],
      "metadata": {
        "id": "ApgifimQSMRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "M8Y8a54XQAGD"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset_train, shuffle = True, batch_size = 32, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(dataset_val, shuffle = True, batch_size = 32,num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, shuffle = True, batch_size = 32,num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graphs of different Audio features:"
      ],
      "metadata": {
        "id": "8_42rW-Ia2Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to plot audio waveform\n",
        "def plot_waveform(waveform, label, sr):\n",
        "    waveform = waveform.detach().numpy()\n",
        "    plt.figure(figsize= (6,2))\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.plot(waveform)\n",
        "    plt.title('Real Audio Waveform' if label == 1 else 'Fake Audio Waveform')\n",
        "    plt.show()\n",
        "\n",
        "#fucntion to plot Mel Spectrogram of Audio file\n",
        "def plot_spectrogram(waveform,label, sr):\n",
        "    waveform = waveform.detach().numpy()\n",
        "    n_fft = 2048\n",
        "    hop_length = 512\n",
        "    win_length = 2048\n",
        "    n_mels = 128\n",
        "\n",
        "    spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length, n_mels=n_mels)\n",
        "    log_spectrogram = librosa.power_to_db(spectrogram)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(log_spectrogram, cmap='viridis', aspect='auto', origin='lower')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Real Audio Spectrogram' if label == 1 else 'Fake Audio Spectrogram')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "#function to plot MFCC of audio file\n",
        "def plot_mfcc(waveform,label,sr):\n",
        "    #mfcc transform that gives us the MFCC tensor\n",
        "    n_fft = 2048\n",
        "    win_length = None\n",
        "    hop_length = 512\n",
        "    n_mels = 256\n",
        "    mfcc_transform = AT.MFCC(\n",
        "        sample_rate=16000,\n",
        "        n_mfcc=40,\n",
        "        melkwargs={\n",
        "            \"n_fft\": n_fft,\n",
        "            \"n_mels\": n_mels,\n",
        "            \"hop_length\": hop_length,\n",
        "            \"mel_scale\": \"htk\",\n",
        "        },\n",
        "    )\n",
        "    mfcc = mfcc_transform(waveform)\n",
        "    if mfcc.shape[1] != 972:\n",
        "        mfcc = torch.nn.functional.pad(mfcc, (0, 972 - mfcc.shape[1], 0, 0), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :972]\n",
        "    mfcc = mfcc.detach().numpy()\n",
        "    log_mfcc = librosa.power_to_db(mfcc)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(log_mfcc, cmap='viridis', aspect='auto', origin='lower')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Real Audio MFCC' if label == 1 else 'Fake Audio MFCC')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('MFCC')\n",
        "    plt.show()\n",
        "\n",
        "def plot_graphs(file_path,label): #0 for fake and 1 for real\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    #to convert stereo to mono\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(signal, dim = 0, keepdims = True)\n",
        "\n",
        "    #calling the ploting functions\n",
        "    plot_waveform(waveform[0],label,16000)\n",
        "    plot_spectrogram(waveform[0],label,16000)\n",
        "    plot_mfcc(waveform[0],label,16000)\n",
        "\n",
        "file_path = \"/kaggle/input/privatedata/Prototype Assignment/fake/Fake_1.wav\"\n",
        "plot_graphs(file_path,0)"
      ],
      "metadata": {
        "id": "5f0uto7hbFd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtxX5yecY7IE"
      },
      "source": [
        "##Training setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-16T19:09:57.181347Z",
          "iopub.status.busy": "2024-05-16T19:09:57.180660Z",
          "iopub.status.idle": "2024-05-16T19:09:57.185621Z",
          "shell.execute_reply": "2024-05-16T19:09:57.184569Z",
          "shell.execute_reply.started": "2024-05-16T19:09:57.181317Z"
        },
        "trusted": true,
        "id": "ghFJI2SwQAGE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#clear cache befor starting the training\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-16T19:13:05.474127Z",
          "iopub.status.busy": "2024-05-16T19:13:05.473779Z",
          "iopub.status.idle": "2024-05-16T19:59:14.062247Z",
          "shell.execute_reply": "2024-05-16T19:59:14.061284Z",
          "shell.execute_reply.started": "2024-05-16T19:13:05.474103Z"
        },
        "id": "sIjimpmFY_k3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = ShallowCNN(in_features=1, out_dim=1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for batch_idx, (batch_mfccs, batch_labels) in loop:\n",
        "        loop.set_description(f'Epoch {epoch + 1} / {num_epochs}')\n",
        "        batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        batch_mfccs = batch_mfccs.squeeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_mfccs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate training loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch_mfccs, batch_labels) in enumerate(val_loader):\n",
        "            batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            batch_mfccs = batch_mfccs.squeeze(1)\n",
        "            outputs = model(batch_mfccs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "            batch_pred = (torch.sigmoid(outputs) + 0.5).int()\n",
        "            # Compute accuracy\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (batch_pred == batch_labels).sum().item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2%}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the model weights in a file:"
      ],
      "metadata": {
        "id": "wAVH1UrMUC-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'FinalDeliverableModel.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to '{model_path}'.\")"
      ],
      "metadata": {
        "id": "lFrVszS3UGsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model:"
      ],
      "metadata": {
        "id": "s0w9wL9uUWeW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-16T20:36:18.283351Z",
          "iopub.status.busy": "2024-05-16T20:36:18.282972Z",
          "iopub.status.idle": "2024-05-16T20:36:18.561467Z",
          "shell.execute_reply": "2024-05-16T20:36:18.560576Z",
          "shell.execute_reply.started": "2024-05-16T20:36:18.283322Z"
        },
        "trusted": true,
        "id": "-g3Lp-raQAGG"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "val_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (batch_mfccs, batch_labels) in tqdm(enumerate(test_loader)):\n",
        "        batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.unsqueeze(1).type(torch.float32).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        batch_mfccs = batch_mfccs.squeeze(1)\n",
        "        outputs = model(batch_mfccs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Accumulate validation loss\n",
        "        val_loss += loss.item()\n",
        "        batch_pred = (torch.sigmoid(outputs) + 0.5).int()\n",
        "        # Compute accuracy\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (batch_pred == batch_labels).sum().item()\n",
        "print(f\"Loss is {val_loss}\")\n",
        "print(correct)\n",
        "print(total)\n",
        "print(f\"Accuracy is {correct/total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model on prototype phase data:"
      ],
      "metadata": {
        "id": "x0DrjhN7VAz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-16T20:42:38.588106Z",
          "iopub.status.busy": "2024-05-16T20:42:38.587739Z",
          "iopub.status.idle": "2024-05-16T20:42:39.339318Z",
          "shell.execute_reply": "2024-05-16T20:42:39.338357Z",
          "shell.execute_reply.started": "2024-05-16T20:42:38.588079Z"
        },
        "trusted": true,
        "id": "jlCYlxmeQAGG"
      },
      "outputs": [],
      "source": [
        "prototype_phase_test_loader = torch.utils.data.DataLoader(prototype_phase_data, batch_size = 1)\n",
        "def inference_function(prototype_phase_test_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch_mfccs, batch_labels) in enumerate(tqdm(prototype_phase_test_loader)):\n",
        "            batch_mfccs, batch_labels = batch_mfccs.to(device), batch_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            batch_mfccs = batch_mfccs.squeeze(1)\n",
        "            outputs = model(batch_mfccs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "            print(f\"For {batch_idx} we have the label {batch_labels[0]} and the predicted output is {predicted[0]}\")\n",
        "\n",
        "inference_function(test_DataLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEejq2EKiGBl"
      },
      "source": [
        "##Inference on a single file:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''For inference on a single file make sure the weights files is in the same directory as the notebook.\n",
        " And run the first two cells to import libraries and the Model class cell to define the model'''\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "model = ShallowCNN(in_features=1, out_dim=1).to(device)\n",
        "state_dict = torch.load('FinalDeliverableModel.pth')\n",
        "model.load_state_dict(state_dict)\n",
        "print(\"model loaded Successfuly!\")\n",
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 256\n",
        "\n",
        "#mfcc transform that gives us the MFCC tensor\n",
        "mfcc_transform = AT.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=40,\n",
        "    melkwargs={\n",
        "        \"n_fft\": n_fft,\n",
        "        \"n_mels\": n_mels,\n",
        "        \"hop_length\": hop_length,\n",
        "        \"mel_scale\": \"htk\",\n",
        "    },\n",
        ")\n",
        "\n",
        "#Function to preprocess audio data:\n",
        "def preprocess_audio(file_path,transform = mfcc_transform, target_time_length = 972):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    waveform = mix_down_if_necessary(waveform)\n",
        "    if sample_rate != 16000:\n",
        "        resampler_transform = AT.Resample(sample_rate,16000)\n",
        "        waveform = resampler_transform(waveform)\n",
        "        sample_rate = 16000\n",
        "\n",
        "    # Apply MFCC transformation\n",
        "    if transform:\n",
        "        mfcc = transform(waveform)\n",
        "        if mfcc.shape[2] != target_time_length:\n",
        "            mfcc = torch.nn.functional.pad(mfcc, (0, target_time_length - mfcc.shape[2], 0, 0), mode='constant')\n",
        "        else:\n",
        "            mfcc = mfcc[:, :target_time_length]\n",
        "\n",
        "    return mfcc\n",
        "\n",
        "def mix_down_if_necessary(signal): #converting from stereo to mono\n",
        "    if signal.shape[0] > 1:\n",
        "        signal = torch.mean(signal, dim = 0, keepdims = True)\n",
        "    return signal\n",
        "\n",
        "# Prompt user to upload audio file. NOTE: Below code is only for google colab.\n",
        "print(\"Upload an audio file:\")\n",
        "uploaded = files.upload()\n",
        "file_path = next(iter(uploaded.keys()))\n",
        "\n",
        "# Preprocess the provided file\n",
        "mfccs_tensor = preprocess_audio(file_path)\n",
        "mfccs_tensor = mfccs_tensor.cuda()\n",
        "\n",
        "# Perform inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(mfccs_tensor)\n",
        "    predicted = (torch.sigmoid(output) + 0.5).int()\n",
        "\n",
        "# Display prediction\n",
        "if predicted.item() == 1:\n",
        "    print(f\"The model predicts that file is real.\")\n",
        "else:\n",
        "    print(f\"The model predicts that file is AI-generated.\")\n"
      ],
      "metadata": {
        "id": "Uad4xj0_QAGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "c4a54382-a774-4d5c-f06b-747f7059f1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded Successfuly!\n",
            "Upload an audio file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-81e81823-1149-4699-b2aa-30855bf5c0bb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-81e81823-1149-4699-b2aa-30855bf5c0bb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Fake_5.wav to Fake_5.wav\n",
            "The model predicts that file is AI-generated.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 1159053,
          "sourceId": 1942970,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2286778,
          "sourceId": 3842332,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3084682,
          "sourceId": 5306083,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3579787,
          "sourceId": 6358196,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4555568,
          "sourceId": 8130934,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4827121,
          "sourceId": 8324965,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4960207,
          "sourceId": 8349132,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}